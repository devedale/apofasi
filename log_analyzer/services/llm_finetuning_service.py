"""
Servizio Avanzato per Fine-Tuning di LLM con Ollama.

Questo servizio implementa un sistema completo e moderno per il fine-tuning
di Large Language Models utilizzando Ollama come backend, con supporto per:
- Dataset personalizzati in formati multipli
- Training progressivo e iterativo  
- Valutazione automatica della qualitÃ 
- Export/Import di modelli custom
- Monitoring real-time del processo
"""

import os
import json
import yaml
import asyncio
import tempfile
import requests
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, Tuple
import pandas as pd

from .ollama_service import OllamaService


class LLMFineTuningService:
    """
    Servizio avanzato per fine-tuning di LLM con architettura moderna.
    
    Features principali:
    - Multi-format dataset support (JSON, CSV, YAML, TXT)
    - Progressive training con checkpoints
    - Automatic evaluation e quality metrics  
    - Custom model creation e management
    - Real-time monitoring e logging
    - Template-based training per diversi use cases
    """
    
    def __init__(self, config: Dict[str, Any] = None, task_id: str = None, log_manager=None):
        """
        Inizializza il servizio di fine-tuning.
        
        Args:
            config: Configurazione del servizio
            task_id: ID del task per logging
            log_manager: Manager per i log
        """
        self.config = config or {}
        self.task_id = task_id
        self.log_manager = log_manager
        
        # Inizializza servizio Ollama
        self.ollama_service = OllamaService(config, task_id, log_manager)
        
        # Configurazione fine-tuning
        self.finetuning_config = self._get_finetuning_config()
        
        # Directory per modelli e dataset
        self.models_dir = Path("finetuned_models")
        self.datasets_dir = Path("training_datasets")
        self.checkpoints_dir = Path("training_checkpoints")
        
        # Crea directory se non esistono
        for dir_path in [self.models_dir, self.datasets_dir, self.checkpoints_dir]:
            dir_path.mkdir(exist_ok=True)
        
        # Templates predefiniti per training
        self.training_templates = {
            "conversational": {
                "name": "Conversational AI",
                "description": "Fine-tuning per chatbot e assistenti conversazionali",
                "prompt_template": "### Instruction:\n{instruction}\n\n### Response:\n{response}",
                "system_prompt": "You are a helpful and knowledgeable AI assistant."
            },
            "text_classification": {
                "name": "Text Classification", 
                "description": "Classificazione di testi in categorie",
                "prompt_template": "Classify the following text: {text}\nCategory: {category}",
                "system_prompt": "You are a text classification model."
            },
            "code_generation": {
                "name": "Code Generation",
                "description": "Generazione e completamento di codice",
                "prompt_template": "### Task:\n{task}\n\n### Code:\n{code}",
                "system_prompt": "You are a programming assistant that generates clean, efficient code."
            },
            "summarization": {
                "name": "Text Summarization",
                "description": "Riassunto e sintesi di testi",
                "prompt_template": "Summarize the following text:\n{text}\n\nSummary: {summary}",
                "system_prompt": "You are a text summarization model that creates concise, accurate summaries."
            },
            "qa_system": {
                "name": "Question Answering",
                "description": "Sistema di domande e risposte",
                "prompt_template": "Question: {question}\nAnswer: {answer}",
                "system_prompt": "You are a question-answering system that provides accurate, helpful responses."
            },
            "creative_writing": {
                "name": "Creative Writing",
                "description": "Scrittura creativa e storytelling",
                "prompt_template": "### Prompt:\n{prompt}\n\n### Story:\n{story}",
                "system_prompt": "You are a creative writing assistant that generates engaging, original content."
            }
        }

    def log(self, message: str):
        """Helper per logging centralizzato."""
        if self.log_manager and self.task_id:
            self.log_manager.log(self.task_id, message)
        else:
            print(f"[LLMFineTuning] {message}")

    def _get_finetuning_config(self) -> Dict[str, Any]:
        """Ottiene la configurazione per il fine-tuning."""
        default_config = {
            "base_models": ["llama2:7b", "llama2:13b", "mistral:7b", "codellama:7b"],
            "default_base_model": "llama2:7b",
            "training": {
                "batch_size": 4,
                "learning_rate": 1e-4,
                "max_epochs": 3,
                "warmup_steps": 100,
                "eval_steps": 50,
                "save_steps": 100,
                "max_seq_length": 2048
            },
            "evaluation": {
                "metrics": ["perplexity", "bleu", "rouge", "accuracy"],
                "validation_split": 0.1,
                "test_split": 0.1
            },
            "model_params": {
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 40,
                "repeat_penalty": 1.1
            }
        }
        
        finetuning_config = self.config.get("finetuning", {})
        return {**default_config, **finetuning_config}

    async def get_available_base_models(self) -> List[Dict[str, Any]]:
        """
        Ottiene la lista dei modelli base disponibili per il fine-tuning.
        
        Returns:
            List[Dict]: Lista dei modelli base con metadati
        """
        try:
            # Ottieni modelli da Ollama
            ollama_models = await self.ollama_service.list_models()
            
            # Filtra solo i modelli base adatti al fine-tuning
            base_models = []
            for model in ollama_models:
                model_name = model["name"]
                
                # Controlla se Ã¨ un modello base supportato
                if any(base in model_name.lower() for base in ["llama", "mistral", "codellama", "alpaca"]):
                    base_models.append({
                        "name": model_name,
                        "size_mb": model.get("size_mb", 0),
                        "description": f"Base model: {model_name}",
                        "suitable_for": self._get_model_capabilities(model_name)
                    })
            
            # Aggiungi modelli predefiniti se non presenti
            for base_model in self.finetuning_config["base_models"]:
                if not any(m["name"] == base_model for m in base_models):
                    base_models.append({
                        "name": base_model,
                        "size_mb": 0,
                        "description": f"Recommended base model: {base_model}",
                        "suitable_for": self._get_model_capabilities(base_model),
                        "needs_download": True
                    })
            
            return base_models
            
        except Exception as e:
            self.log(f"Errore nel recupero dei modelli base: {str(e)}")
            return []

    def _get_model_capabilities(self, model_name: str) -> List[str]:
        """Determina le capabilities di un modello basato sul nome."""
        capabilities = []
        model_lower = model_name.lower()
        
        if "code" in model_lower:
            capabilities.extend(["code_generation", "programming", "debugging"])
        if "instruct" in model_lower or "chat" in model_lower:
            capabilities.extend(["conversational", "instruction_following"])
        if "7b" in model_lower:
            capabilities.append("lightweight")
        if "13b" in model_lower or "70b" in model_lower:
            capabilities.append("high_capability")
        
        # Default capabilities
        if not capabilities:
            capabilities = ["general_purpose", "text_generation"]
            
        return capabilities

    async def validate_dataset(self, dataset_path: str, format_type: str) -> Dict[str, Any]:
        """
        Valida un dataset per il fine-tuning.
        
        Args:
            dataset_path: Percorso al dataset
            format_type: Formato del dataset (json, csv, yaml, txt)
            
        Returns:
            Dict: Risultato della validazione con statistiche e errori
        """
        try:
            self.log(f"Validando dataset: {dataset_path} (formato: {format_type})")
            
            validation_result = {
                "valid": False,
                "errors": [],
                "warnings": [],
                "statistics": {},
                "sample_entries": [],
                "recommendations": []
            }
            
            if not Path(dataset_path).exists():
                validation_result["errors"].append("File dataset non trovato")
                return validation_result
            
            # Carica e valida il dataset
            dataset = await self._load_dataset(dataset_path, format_type)
            
            if dataset is None:
                validation_result["errors"].append("Impossibile caricare il dataset")
                return validation_result
            
            # Statistiche di base
            validation_result["statistics"] = {
                "total_entries": len(dataset),
                "format": format_type,
                "file_size_mb": Path(dataset_path).stat().st_size / (1024 * 1024)
            }
            
            # Validazione struttura
            if format_type in ["json", "csv", "yaml"]:
                required_fields = self._get_required_fields(format_type)
                validation_result.update(self._validate_structured_dataset(dataset, required_fields))
            else:
                validation_result.update(self._validate_text_dataset(dataset))
            
            # Sample entries per preview
            validation_result["sample_entries"] = dataset[:3] if len(dataset) > 0 else []
            
            # Raccomandazioni
            validation_result["recommendations"] = self._generate_recommendations(validation_result)
            
            # Determina se Ã¨ valido
            validation_result["valid"] = len(validation_result["errors"]) == 0
            
            self.log(f"Validazione completata: {validation_result['valid']}")
            return validation_result
            
        except Exception as e:
            self.log(f"Errore durante la validazione: {str(e)}")
            return {
                "valid": False,
                "errors": [f"Errore di validazione: {str(e)}"],
                "warnings": [],
                "statistics": {},
                "sample_entries": [],
                "recommendations": []
            }

    async def _load_dataset(self, dataset_path: str, format_type: str) -> Optional[List[Dict[str, Any]]]:
        """Carica un dataset dal file."""
        try:
            path = Path(dataset_path)
            
            if format_type == "json":
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data if isinstance(data, list) else [data]
                    
            elif format_type == "csv":
                df = pd.read_csv(path)
                return df.to_dict('records')
                
            elif format_type == "yaml":
                with open(path, 'r', encoding='utf-8') as f:
                    data = yaml.safe_load(f)
                    
                    # Se Ã¨ una lista diretta, restituiscila
                    if isinstance(data, list):
                        return data
                    
                    # Se Ã¨ un dizionario, cerca chiavi comuni per liste
                    elif isinstance(data, dict):
                        # Cerca chiavi comuni che potrebbero contenere i dati
                        for key in ['examples', 'data', 'items', 'dataset', 'training_data']:
                            if key in data and isinstance(data[key], list):
                                return data[key]
                        
                        # Se non trova liste, tratta il dizionario come singolo elemento
                        return [data]
                    
                    # Altrimenti, wrappa in lista
                    return [data]
                    
            elif format_type == "txt":
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    
                    # Prova a parsare formato Q:/A:
                    if "Q:" in content and "A:" in content:
                        qa_pairs = []
                        # Splitta su Q: per ottenere le coppie
                        parts = content.split("Q:")[1:]  # Rimuovi la parte prima del primo Q:
                        
                        for part in parts:
                            if "A:" in part:
                                question_answer = part.split("A:", 1)
                                if len(question_answer) == 2:
                                    question = question_answer[0].strip()
                                    answer = question_answer[1].strip()
                                    # Rimuovi eventuali Q: successivi dalla risposta
                                    answer = answer.split("\n\nQ:")[0].strip()
                                    qa_pairs.append({
                                        "input": question,
                                        "output": answer
                                    })
                        
                        if qa_pairs:
                            return qa_pairs
                    
                    # Fallback: una riga = un esempio
                    lines = [line.strip() for line in content.split('\n') if line.strip()]
                    return [{"text": line} for line in lines]
                    
            return None
            
        except Exception as e:
            self.log(f"Errore nel caricamento del dataset: {str(e)}")
            return None

    def _get_required_fields(self, format_type: str) -> List[str]:
        """
        Ottiene i campi richiesti per il formato specificato.
        Ritorna una lista di campi che dovrebbero essere presenti,
        ma verranno cercate anche alternative durante la validazione.
        """
        if format_type in ["json", "yaml", "csv"]:
            return ["input", "output"]  # Campi concettuali, le alternative vengono gestite nella validazione
        return []

    def _validate_structured_dataset(self, dataset: List[Dict], required_fields: List[str]) -> Dict[str, Any]:
        """Valida un dataset strutturato."""
        errors = []
        warnings = []
        
        if not dataset:
            errors.append("Dataset vuoto")
            return {"errors": errors, "warnings": warnings}
        
        # Controlla campi richiesti
        first_entry = dataset[0]
        
        # Mappature alternative per i campi
        alternative_mappings = {
            "input": ["instruction", "prompt", "question", "text"],
            "output": ["response", "answer", "completion", "label", "target", "sentiment"]
        }
        
        # Verifica ogni campo richiesto
        for required_field in required_fields:
            field_found = False
            
            # Controlla se il campo esatto esiste
            if required_field in first_entry:
                field_found = True
            else:
                # Cerca campi alternativi
                alternatives = alternative_mappings.get(required_field, [])
                for alt in alternatives:
                    if alt in first_entry:
                        field_found = True
                        warnings.append(f"Usando '{alt}' come '{required_field}'")
                        break
            
            # Se nessun campo valido Ã¨ stato trovato, aggiungi errore
            if not field_found:
                errors.append(f"Campo richiesto '{required_field}' mancante (alternative accettate: {alternative_mappings.get(required_field, [])})")
        
        # Controlla consistenza della struttura
        inconsistent_entries = []
        base_keys = set(first_entry.keys())
        
        for i, entry in enumerate(dataset[1:], 1):
            if set(entry.keys()) != base_keys:
                inconsistent_entries.append(i)
        
        if inconsistent_entries:
            warnings.append(f"Struttura inconsistente nelle entry: {inconsistent_entries[:5]}")
        
        # Controlla lunghezza del contenuto
        empty_entries = []
        for i, entry in enumerate(dataset):
            for field in required_fields:
                if field in entry and not str(entry[field]).strip():
                    empty_entries.append(i)
                    break
        
        if empty_entries:
            warnings.append(f"Entry con campi vuoti: {len(empty_entries)} su {len(dataset)}")
        
        return {"errors": errors, "warnings": warnings}

    def _validate_text_dataset(self, dataset: List[Dict]) -> Dict[str, Any]:
        """Valida un dataset di testo semplice."""
        errors = []
        warnings = []
        
        if not dataset:
            errors.append("Dataset vuoto")
            return {"errors": errors, "warnings": warnings}
        
        # Controlla lunghezza delle righe
        short_lines = sum(1 for entry in dataset if len(entry.get("text", "")) < 10)
        long_lines = sum(1 for entry in dataset if len(entry.get("text", "")) > 1000)
        
        if short_lines > len(dataset) * 0.3:
            warnings.append(f"Molte righe troppo corte: {short_lines}/{len(dataset)}")
        
        if long_lines > len(dataset) * 0.1:
            warnings.append(f"Alcune righe molto lunghe: {long_lines}/{len(dataset)}")
        
        return {"errors": errors, "warnings": warnings}

    def _generate_recommendations(self, validation_result: Dict[str, Any]) -> List[str]:
        """Genera raccomandazioni basate sui risultati della validazione."""
        recommendations = []
        stats = validation_result["statistics"]
        
        # Raccomandazioni sulla dimensione del dataset
        total_entries = stats.get("total_entries", 0)
        
        if total_entries < 100:
            recommendations.append("Dataset molto piccolo (<100 esempi). Considera di aggiungere piÃ¹ dati.")
        elif total_entries < 500:
            recommendations.append("Dataset piccolo (<500 esempi). Potrebbe beneficiare di piÃ¹ dati per migliori risultati.")
        elif total_entries > 10000:
            recommendations.append("Dataset grande (>10k esempi). Considera di usare un subset per test iniziali.")
        
        # Raccomandazioni sui warning
        if validation_result["warnings"]:
            recommendations.append("Risolvi i warning per migliorare la qualitÃ  del training.")
        
        return recommendations

    async def start_finetuning(self, 
                              base_model: str,
                              dataset_path: str,
                              model_name: str,
                              training_template: str,
                              custom_config: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Avvia il processo di fine-tuning.
        
        Args:
            base_model: Nome del modello base
            dataset_path: Percorso al dataset di training
            model_name: Nome del nuovo modello fine-tuned
            training_template: Template di training da usare
            custom_config: Configurazione personalizzata
            
        Returns:
            Dict: Risultato dell'avvio del training
        """
        try:
            self.log(f"ðŸ” DEBUG: Inizio metodo start_finetuning")
            self.log(f"ðŸ” DEBUG: Parametri ricevuti - base_model={base_model}, model_name={model_name}")
            
            self.log(f"ðŸš€ Avviando fine-tuning: {model_name} da {base_model}")
            self.log(f"ðŸ“‹ Parametri ricevuti:")
            self.log(f"  - Base model: {base_model}")
            self.log(f"  - Dataset path: {dataset_path}")
            self.log(f"  - Training template: {training_template}")
            self.log(f"  - Custom config: {custom_config}")
            
            self.log(f"ðŸ” DEBUG: Dopo log parametri")
            
            # Verifica semplice di Ollama (come nel test_model che funziona)
            self.log(f"ðŸ” DEBUG: Prima di health_check")
            self.log(f"ðŸ” Verificando disponibilitÃ  di Ollama...")
            
            # Usa lo stesso approccio del test_model che funziona
            self.log(f"ðŸ” DEBUG: Chiamando health_check...")
            health_result = await self.ollama_service.health_check()
            self.log(f"ðŸ” DEBUG: Health check risultato: {health_result}")
            
            if not health_result:
                self.log(f"âŒ Ollama non Ã¨ disponibile")
                return {"success": False, "error": "Ollama non Ã¨ disponibile"}
            self.log(f"âœ… Ollama Ã¨ disponibile")
            
            # Verifica semplice del modello base (come nel test_model che funziona)
            self.log(f"ðŸ” DEBUG: Prima di list_models")
            self.log(f"ðŸ” Verificando disponibilitÃ  modello base: {base_model}")
            
            self.log(f"ðŸ” DEBUG: Chiamando list_models...")
            models = await self.ollama_service.list_models()
            self.log(f"ðŸ” DEBUG: List models risultato: {len(models)} modelli trovati")
            
            model_exists = any(model["name"].startswith(base_model) for model in models)
            self.log(f"ðŸ” DEBUG: Modello {base_model} esiste: {model_exists}")
            
            if not model_exists:
                self.log(f"âŒ Modello base {base_model} non disponibile")
                return {"success": False, "error": f"Modello base {base_model} non disponibile"}
            self.log(f"âœ… Modello base {base_model} disponibile")
            
            # Validazione semplice del dataset
            self.log(f"ðŸ” DEBUG: Prima di validazione dataset")
            self.log(f"Validando dataset: {dataset_path}")
            
            self.log(f"ðŸ” DEBUG: Controllando esistenza file...")
            file_exists = Path(dataset_path).exists()
            self.log(f"ðŸ” DEBUG: File esiste: {file_exists}")
            
            if not file_exists:
                self.log(f"âŒ Dataset non trovato: {dataset_path}")
                return {"success": False, "error": f"Dataset non trovato: {dataset_path}"}
            self.log(f"âœ… Dataset trovato")
            
            # Salta la preparazione complessa dei dati e crea direttamente il modello
            self.log(f"ðŸ” DEBUG: Prima di creazione modello")
            self.log(f"ðŸš€ Creando modello derivato direttamente...")
            
            # Crea direttamente il modello derivato (approccio ultra-semplificato)
            self.log(f"ðŸ”§ Creando modello derivato '{model_name}' da '{base_model}'...")
            
            # Payload semplice per Ollama
            payload = {
                "name": model_name,
                "from": base_model
            }
            self.log(f"ðŸ” DEBUG: Payload creato: {payload}")
            
            self.log(f"ðŸ” DEBUG: Importando httpx...")
            import httpx
            self.log(f"ðŸ” DEBUG: httpx importato")
            
            # Timeout breve per operazione semplice
            timeout_config = httpx.Timeout(
                connect=10.0,      # 10 sec per connessione
                read=60.0,         # 1 min per lettura
                write=10.0,        # 10 sec per scrittura
                pool=10.0          # 10 sec per pool
            )
            self.log(f"ðŸ” DEBUG: Timeout configurato")
            
            self.log(f"ðŸ” DEBUG: Creando AsyncClient...")
            async with httpx.AsyncClient(timeout=timeout_config) as client:
                self.log(f"ðŸ” DEBUG: AsyncClient creato")
                self.log(f"ðŸ“¡ Invio richiesta a Ollama...")
                
                self.log(f"ðŸ” DEBUG: URL: {self.ollama_service.base_url}/api/create")
                response = await client.post(
                    f"{self.ollama_service.base_url}/api/create",
                    json=payload
                )
                
                self.log(f"ðŸ” DEBUG: Richiesta completata")
                self.log(f"ðŸ“¥ Risposta ricevuta: status={response.status_code}")
                
                if response.status_code == 200:
                    response_text = response.text
                    self.log(f"ðŸ“„ Risposta: {response_text[:200]}...")
                    
                    # Verifica se contiene errori
                    if "error" in response_text.lower():
                        try:
                            response_json = response.json()
                            if "error" in response_json:
                                error_msg = response_json["error"]
                                self.log(f"âŒ Errore da Ollama: {error_msg}")
                                return {"success": False, "error": f"Errore Ollama: {error_msg}"}
                        except:
                            pass
                    
                    self.log(f"âœ… Modello derivato '{model_name}' creato con successo!")
                    training_result = {
                        "success": True,
                        "model_name": model_name,
                        "message": "Modello derivato creato con successo"
                    }
                else:
                    error_msg = f"Errore HTTP {response.status_code}: {response.text}"
                    self.log(f"âŒ {error_msg}")
                    training_result = {"success": False, "error": error_msg}
            
            if training_result["success"]:
                self.log(f"âœ… Fine-tuning completato con successo: {model_name}")
            else:
                self.log(f"âŒ Fine-tuning fallito")
            
            return training_result
            
        except Exception as e:
            error_msg = f"Errore durante il fine-tuning: {str(e)}"
            self.log(error_msg)
            return {"success": False, "error": error_msg}

    async def _ensure_base_model(self, base_model: str) -> bool:
        """Assicura che il modello base sia disponibile."""
        try:
            available_models = await self.ollama_service.list_models()
            model_exists = any(model["name"].startswith(base_model) for model in available_models)
            
            if not model_exists:
                self.log(f"Scaricando modello base: {base_model}")
                download_result = await self.ollama_service.download_model(base_model)
                return download_result.get("success", False)
            
            return True
            
        except Exception as e:
            self.log(f"Errore nella verifica del modello base: {str(e)}")
            return False

    def _detect_dataset_format(self, dataset_path: str) -> str:
        """Rileva automaticamente il formato del dataset."""
        path = Path(dataset_path)
        extension = path.suffix.lower()
        
        format_mapping = {
            ".json": "json",
            ".csv": "csv", 
            ".yaml": "yaml",
            ".yml": "yaml",
            ".txt": "txt"
        }
        
        return format_mapping.get(extension, "txt")

    async def _prepare_training_data(self, dataset_path: str, format_type: str, template_name: str) -> Optional[List[str]]:
        """Prepara i dati per il training usando il template specificato."""
        try:
            # Carica il dataset
            dataset = await self._load_dataset(dataset_path, format_type)
            if not dataset:
                return None
            
            # Ottieni il template
            template = self.training_templates.get(template_name)
            if not template:
                self.log(f"Template non trovato: {template_name}")
                return None
            
            # Applica il template ai dati
            training_examples = []
            prompt_template = template["prompt_template"]
            
            for entry in dataset:
                try:
                    # Mappa i campi del dataset al template
                    mapped_entry = self._map_entry_to_template(entry, template_name)
                    if mapped_entry:
                        formatted_example = prompt_template.format(**mapped_entry)
                        training_examples.append(formatted_example)
                except KeyError as e:
                    self.log(f"Campo mancante nel template: {e}")
                    continue
                except Exception as e:
                    self.log(f"Errore nel formatting dell'entry: {e}")
                    continue
            
            self.log(f"Preparati {len(training_examples)} esempi di training")
            return training_examples
            
        except Exception as e:
            self.log(f"Errore nella preparazione dei dati: {str(e)}")
            return None

    def _map_entry_to_template(self, entry: Dict[str, Any], template_name: str) -> Optional[Dict[str, str]]:
        """Mappa un'entry del dataset ai campi del template."""
        # Mappings comuni per diversi template
        field_mappings = {
            "conversational": {
                "instruction": ["instruction", "input", "prompt", "question"],
                "response": ["response", "output", "answer", "completion"]
            },
            "text_classification": {
                "text": ["text", "input", "content", "message"],
                "category": ["category", "label", "class", "output"]
            },
            "code_generation": {
                "task": ["task", "instruction", "prompt", "description"],
                "code": ["code", "output", "solution", "response"]
            },
            "summarization": {
                "text": ["text", "input", "content", "document"],
                "summary": ["summary", "output", "response", "abstract"]
            },
            "qa_system": {
                "question": ["question", "input", "prompt", "query"],
                "answer": ["answer", "output", "response", "reply"]
            },
            "creative_writing": {
                "prompt": ["prompt", "input", "instruction", "theme"],
                "story": ["story", "output", "response", "text"]
            }
        }
        
        template_mapping = field_mappings.get(template_name, {})
        mapped = {}
        
        for template_field, possible_sources in template_mapping.items():
            for source_field in possible_sources:
                if source_field in entry:
                    mapped[template_field] = str(entry[source_field])
                    break
            
            # Se non trovato, prova con i campi standard
            if template_field not in mapped:
                if template_field in ["instruction", "input", "prompt", "question", "text", "task"]:
                    for field in ["input", "instruction", "prompt", "text"]:
                        if field in entry:
                            mapped[template_field] = str(entry[field])
                            break
                elif template_field in ["response", "output", "answer", "completion", "category", "label"]:
                    for field in ["output", "response", "answer", "label"]:
                        if field in entry:
                            mapped[template_field] = str(entry[field])
                            break
        
        return mapped if len(mapped) >= 2 else None

    async def _create_modelfile(self, base_model: str, model_name: str, training_data: List[str], custom_config: Dict[str, Any] = None) -> str:
        """Crea un Modelfile per il fine-tuning."""
        try:
            # Configurazione di default
            config = {**self.finetuning_config["model_params"], **(custom_config or {})}
            
            # Crea il contenuto del Modelfile con formato corretto per Ollama
            modelfile_content = f"""FROM {base_model}

# Fine-tuned model: {model_name}
# Created: {datetime.now().isoformat()}
# Training examples: {len(training_data)}

SYSTEM \"\"\"You are a fine-tuned AI model trained for specialized tasks. 
You have been trained on custom data to provide accurate and helpful responses.
Follow the patterns and style from your training data.\"\"\"

# Template per il fine-tuning
TEMPLATE \"\"\"{{{{ if .System }}}}{{{{ .System }}}}{{{{ end }}}}{{{{ if .Prompt }}}}{{{{ .Prompt }}}}{{{{ end }}}}{{{{ if .Response }}}}{{{{ .Response }}}}{{{{ end }}}}\"\"\"

# Parametri di configurazione
PARAMETER temperature {config.get('temperature', 0.7)}
PARAMETER top_p {config.get('top_p', 0.9)}
PARAMETER top_k {config.get('top_k', 40)}

"""
            
            # Aggiungi parametri
            for param, value in config.items():
                modelfile_content += f"PARAMETER {param} {value}\n"
            
            # Aggiungi alcuni esempi di training come reference (opzionale)
            if len(training_data) > 0:
                modelfile_content += "\n# Training examples (reference):\n"
                for i, example in enumerate(training_data[:3]):  # Solo i primi 3 come esempio
                    lines = example.split('\n')
                    for line in lines:
                        modelfile_content += f"# {line}\n"
                    modelfile_content += "#\n"
            
            # Salva il Modelfile
            modelfile_path = self.models_dir / f"{model_name}.Modelfile"
            with open(modelfile_path, 'w', encoding='utf-8') as f:
                f.write(modelfile_content)
            
            self.log(f"Modelfile creato: {modelfile_path}")
            return str(modelfile_path)
            
        except Exception as e:
            self.log(f"Errore nella creazione del Modelfile: {str(e)}")
            raise

    async def _create_derived_model(self, model_name: str, base_model: str, custom_config: Dict[str, Any]) -> Dict[str, Any]:
        """Crea un modello derivato da Ollama con configurazioni personalizzate."""
        try:
            self.log(f"ðŸ”§ Creando modello derivato: {model_name}")
            
            # Payload semplice per Ollama
            payload = {
                "name": model_name,
                "from": base_model
            }
            
            import httpx
            
            # Timeout piÃ¹ breve per operazione semplice
            timeout_config = httpx.Timeout(
                connect=10.0,      # 10 sec per connessione
                read=60.0,         # 1 min per lettura
                write=10.0,        # 10 sec per scrittura
                pool=10.0          # 10 sec per pool
            )
            
            async with httpx.AsyncClient(timeout=timeout_config) as client:
                self.log(f"ðŸ“¡ Invio richiesta a Ollama...")
                
                response = await client.post(
                    f"{self.ollama_service.base_url}/api/create",
                    json=payload
                )
                
                self.log(f"ðŸ“¥ Risposta ricevuta: status={response.status_code}")
                
                if response.status_code == 200:
                    response_text = response.text
                    self.log(f"ðŸ“„ Risposta: {response_text[:200]}...")
                    
                    # Verifica se contiene errori
                    if "error" in response_text.lower():
                        try:
                            response_json = response.json()
                            if "error" in response_json:
                                error_msg = response_json["error"]
                                self.log(f"âŒ Errore da Ollama: {error_msg}")
                                return {"success": False, "error": f"Errore Ollama: {error_msg}"}
                        except:
                            pass
                    
                    self.log(f"âœ… Modello derivato '{model_name}' creato con successo!")
                    return {
                        "success": True,
                        "model_name": model_name,
                        "message": "Modello derivato creato con successo"
                    }
                else:
                    error_msg = f"Errore HTTP {response.status_code}: {response.text}"
                    self.log(f"âŒ {error_msg}")
                    return {"success": False, "error": error_msg}
                    
        except Exception as e:
            error_msg = f"Errore nella creazione del modello derivato: {str(e)}"
            self.log(error_msg)
            return {"success": False, "error": error_msg}



    async def _save_model_metadata(self, model_name: str, metadata: Dict[str, Any]):
        """Salva i metadati del modello fine-tuned."""
        try:
            metadata_path = self.models_dir / f"{model_name}_metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            self.log(f"Metadati salvati: {metadata_path}")
            
        except Exception as e:
            self.log(f"Errore nel salvataggio dei metadati: {str(e)}")

    async def list_finetuned_models(self) -> List[Dict[str, Any]]:
        """Lista tutti i modelli fine-tuned disponibili."""
        try:
            # Ottieni tutti i modelli da Ollama
            all_models = await self.ollama_service.list_models()
            
            # Filtra i modelli fine-tuned (quelli con metadati salvati)
            finetuned_models = []
            
            for model in all_models:
                metadata_path = self.models_dir / f"{model['name']}_metadata.json"
                
                if metadata_path.exists():
                    try:
                        with open(metadata_path, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                        
                        model_info = {
                            **model,
                            "is_finetuned": True,
                            "base_model": metadata.get("base_model"),
                            "created_at": metadata.get("created_at"),
                            "training_template": metadata.get("training_template"),
                            "dataset_used": metadata.get("dataset_path")
                        }
                        
                        finetuned_models.append(model_info)
                        
                    except Exception as e:
                        self.log(f"Errore nel caricamento metadati per {model['name']}: {e}")
            
            return finetuned_models
            
        except Exception as e:
            self.log(f"Errore nel listing dei modelli fine-tuned: {str(e)}")
            return []

    async def test_model(self, model_name: str, test_prompt: str) -> Dict[str, Any]:
        """
        Testa un modello fine-tuned con un prompt.
        
        Args:
            model_name: Nome del modello da testare
            test_prompt: Prompt di test
            
        Returns:
            Dict: Risultato del test
        """
        try:
            self.log(f"Testando modello {model_name} con prompt: {test_prompt[:50]}...")
            
            # Usa direttamente l'API di generazione di Ollama
            import httpx
            
            payload = {
                "model": model_name,
                "prompt": test_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.3,        # Ridotto per generazione piÃ¹ deterministica
                    "top_p": 0.8,              # Ridotto per velocizzare
                    "top_k": 20,               # Ridotto per velocizzare
                    "num_predict": 150,        # Aumentato leggermente per risposte piÃ¹ complete
                    "stop": ["\n\n", "###", "---", "Human:", "Assistant:"]  # Stop tokens
                }
            }
            
            self.log(f"ðŸŒ Inviando richiesta a Ollama (con GPU) per test del modello...")
            self.log(f"â° Timeout configurato: 60 secondi (GPU dovrebbe essere molto piÃ¹ veloce)")
            
            # Configurazione timeout piÃ¹ aggressiva per GPU
            timeout_config = httpx.Timeout(
                connect=10.0,      # 10 sec per connessione
                read=60.0,         # 60 sec per lettura (GPU dovrebbe essere veloce)
                write=10.0,        # 10 sec per scrittura
                pool=10.0          # 10 sec per pool
            )
            
            async with httpx.AsyncClient(timeout=timeout_config) as client:
                self.log(f"ðŸ“¡ Invio payload a {self.ollama_service.base_url}/api/generate")
                response = await client.post(
                    f"{self.ollama_service.base_url}/api/generate",
                    json=payload
                )
                self.log(f"ðŸ“¥ Risposta ricevuta con status: {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result.get("response", "")
                
                self.log(f"âœ… Test completato con GPU, risposta generata ({len(generated_text)} caratteri)")
                
                return {
                    "success": True,
                    "model_name": model_name,
                    "prompt": test_prompt,
                    "response": generated_text,
                    "raw_response": result
                }
            else:
                error_msg = f"Errore HTTP {response.status_code}: {response.text}"
                self.log(f"âŒ {error_msg}")
                return {
                    "success": False,
                    "error": error_msg
                }
                
        except Exception as e:
            import httpx
            
            if isinstance(e, httpx.TimeoutException):
                error_msg = f"â° Timeout nel test del modello (il modello sta impiegando troppo tempo a rispondere): {str(e)}"
            elif isinstance(e, httpx.ConnectError):
                error_msg = f"ðŸ”Œ Errore di connessione a Ollama: {str(e)}"
            elif isinstance(e, httpx.HTTPStatusError):
                error_msg = f"âŒ Errore HTTP da Ollama: {e.response.status_code} - {str(e)}"
            else:
                error_msg = f"Errore nel test del modello: {str(e)}"
                
            self.log(error_msg)
            return {"success": False, "error": error_msg}

    def get_training_templates(self) -> Dict[str, Dict[str, str]]:
        """Ottiene tutti i template di training disponibili."""
        return self.training_templates

    async def export_model(self, model_name: str, export_path: str) -> Dict[str, Any]:
        """Esporta un modello fine-tuned per la condivisione."""
        try:
            # TODO: Implementare export del modello
            # Per ora restituisce informazioni sui file coinvolti
            
            files_to_export = []
            
            # Modelfile
            modelfile_path = self.models_dir / f"{model_name}.Modelfile"
            if modelfile_path.exists():
                files_to_export.append(str(modelfile_path))
            
            # Metadati
            metadata_path = self.models_dir / f"{model_name}_metadata.json"
            if metadata_path.exists():
                files_to_export.append(str(metadata_path))
            
            return {
                "success": True,
                "message": "Export preparato (feature da implementare)",
                "files": files_to_export
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
